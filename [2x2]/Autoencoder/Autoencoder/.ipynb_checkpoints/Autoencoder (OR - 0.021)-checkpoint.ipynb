{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtrain = pd.read_csv(\"../TrainingSet (OR - 21).csv\")\n",
    "#Dtrain = Dtrain.iloc[:,0:4].values\n",
    "Dtest = pd.read_csv(\"../TestingSet (OR - 21).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193792, 4)\n",
      "(48448, 4)\n"
     ]
    }
   ],
   "source": [
    "print(Dtrain.shape)\n",
    "print(Dtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outer Race (0.021\")    96896\n",
       "Normal                 96896\n",
       "Name: Bearing, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dtrain[\"Bearing\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outer Race (0.021\")    50.0\n",
       "Normal                 50.0\n",
       "Name: Bearing, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dtrain[\"Bearing\"].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Bearing', 'Fault Diameter', 'DE', 'FE'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dtest.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "labelencoder_train = LabelEncoder()\n",
    "Dtrain[\"Bearing\"] = labelencoder_train.fit_transform(Dtrain[\"Bearing\"])\n",
    "ct = ColumnTransformer([(\"Bearing\", OneHotEncoder(), [0])],    remainder = 'passthrough')\n",
    "Dtrain = ct.fit_transform(Dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.         -0.04965046 -0.09635818]\n",
      " [ 1.          0.          0.         -0.00438092 -0.11957455]\n",
      " [ 1.          0.          0.         -0.00333785 -0.11258909]\n",
      " ...\n",
      " [ 0.          1.          0.021      -0.2399979   0.29523818]\n",
      " [ 0.          1.          0.021      -0.27410928  0.10067273]\n",
      " [ 0.          1.          0.021       0.14578553  0.25353091]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "labelencoder_train = LabelEncoder()\n",
    "Dtest[\"Bearing\"] = labelencoder_train.fit_transform(Dtest[\"Bearing\"])\n",
    "ct = ColumnTransformer([(\"Bearing\", OneHotEncoder(), [0])],    remainder = 'passthrough')\n",
    "Dtest = ct.fit_transform(Dtest)\n",
    "\n",
    "print(Dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(Dtrain))\n",
    "# Random shuffle training data\n",
    "X_train.sample(frac=1)\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(Dtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dropout\n",
    "from keras.layers.core import Dense \n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "seed(10)\n",
    "tf.random.set_seed(10)\n",
    "act_func = 'elu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                192       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 519\n",
      "Trainable params: 519\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input layer:\n",
    "model=Sequential()\n",
    "# First hidden layer, connected to input vector X. \n",
    "model.add(Dense(32, activation=act_func,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                kernel_regularizer=regularizers.l2(0.0),\n",
    "                input_shape=(X_train.shape[1],)\n",
    "               )\n",
    "         )\n",
    "\n",
    "model.add(Dense(2, activation=act_func, kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(32, activation=act_func, kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(X_train.shape[1], kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "# Train model for 100 epochs, batch size of 10: \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 174412 samples, validate on 19380 samples\n",
      "Epoch 1/100\n",
      "174412/174412 [==============================] - 1s 3us/step - loss: 0.0804 - val_loss: 8.2945e-04\n",
      "Epoch 2/100\n",
      "174412/174412 [==============================] - 0s 2us/step - loss: 0.0058 - val_loss: 7.8268e-04\n",
      "Epoch 3/100\n",
      "174412/174412 [==============================] - 0s 3us/step - loss: 5.8251e-04 - val_loss: 7.7854e-04\n",
      "Epoch 4/100\n",
      "174412/174412 [==============================] - 0s 2us/step - loss: 3.7078e-04 - val_loss: 7.6906e-04\n",
      "Epoch 5/100\n",
      "174412/174412 [==============================] - 0s 2us/step - loss: 3.6475e-04 - val_loss: 7.5812e-04\n",
      "Epoch 6/100\n",
      "174412/174412 [==============================] - 0s 3us/step - loss: 3.6001e-04 - val_loss: 7.4608e-04\n",
      "Epoch 7/100\n",
      "174412/174412 [==============================] - 0s 3us/step - loss: 3.5497e-04 - val_loss: 7.3315e-04\n",
      "Epoch 8/100\n",
      "174412/174412 [==============================] - 0s 2us/step - loss: 3.4963e-04 - val_loss: 7.1941e-04\n",
      "Epoch 9/100\n",
      "174412/174412 [==============================] - 0s 2us/step - loss: 3.4399e-04 - val_loss: 7.0488e-04\n",
      "Epoch 10/100\n",
      "174412/174412 [==============================] - 0s 2us/step - loss: 3.3810e-04 - val_loss: 6.8956e-04\n",
      "Epoch 11/100\n",
      "174412/174412 [==============================] - 0s 2us/step - loss: 3.3201e-04 - val_loss: 6.7339e-04\n",
      "Epoch 12/100\n",
      "174412/174412 [==============================] - 0s 3us/step - loss: 3.2584e-04 - val_loss: 6.5645e-04\n",
      "Epoch 13/100\n",
      "174412/174412 [==============================] - 0s 2us/step - loss: 3.1956e-04 - val_loss: 6.3889e-04\n",
      "Epoch 14/100\n",
      "174412/174412 [==============================] - 0s 2us/step - loss: 3.1282e-04 - val_loss: 6.2063e-04\n",
      "Epoch 15/100\n",
      " 29000/174412 [===>..........................] - ETA: 0s - loss: 1.0997e-04"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS=100\n",
    "BATCH_SIZE=1000\n",
    "history=model.fit(X_train,X_train, \n",
    "                  batch_size=BATCH_SIZE, \n",
    "                  epochs=NUM_EPOCHS,\n",
    "                  validation_split=0.1,\n",
    "                  verbose = 1,\n",
    "                  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], 'b', label='Training loss')\n",
    "plt.plot(history.history['val_loss'], 'r', label='Validation loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss, [mse]')\n",
    "plt.ylim([0,.05])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "scored = np.mean(np.abs(X_pred-X_train), axis = 1)\n",
    "plt.figure()\n",
    "g = sns.distplot(scored, bins = 10, kde= True);\n",
    "#g.set(xlim=(0.05,0.10), ylim=(0, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TH = 0.06\n",
    "scored = pd.DataFrame()\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-X_train), axis = 1)\n",
    "scored['Threshold'] = TH\n",
    "scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
    "scored.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_train = model.predict(np.array(X_train))\n",
    "X_pred_train = pd.DataFrame(X_pred_train)\n",
    "\n",
    "scored_train = pd.DataFrame()\n",
    "scored_train['Loss_mae'] = np.mean(np.abs(X_pred_train-X_train), axis = 1)\n",
    "scored_train['Threshold'] = TH\n",
    "scored_train['Anomaly'] = scored_train['Loss_mae'] > scored_train['Threshold']\n",
    "scored = pd.concat([scored_train, scored])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_train['Loss_mae'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored.plot(logy=True,  figsize = (10,6), xlim =[0,5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = scored_train[scored_train['Anomaly'] == True]\n",
    "print(anomalies)\n",
    "print(anomalies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_anomalies = anomalies[anomalies.index <= 96896]\n",
    "print(NB_anomalies)\n",
    "print(NB_anomalies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OR7_anomalies = anomalies[anomalies.index > 96896]\n",
    "print(OR7_anomalies)\n",
    "print(OR7_anomalies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scored_train.index, scored_train.Loss_mae, label='Loss(MAE)');\n",
    "plt.plot(scored_train.index, scored_train.Threshold, label='Threshold')\n",
    "g = sns.scatterplot(x=anomalies.index , y=anomalies.Loss_mae, label='anomaly', color='red')\n",
    "g.set(xlim = (len(scored_train.index)-10000, len(scored_train.index)), ylim = (0, 0.2))\n",
    "plt.title('Anomalies in Outer Race 0.007\"')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
